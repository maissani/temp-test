# Incident Response: INC-2026-01-21-ROUTER-5XX

**Incident:** Erreurs 5xx intermittentes et d√©ploiements lents
**Fen√™tre:** 2026-01-21 08:45‚Äì10:00 CET
**R√©gion:** region-eu-1, cluster-redacted-01

---

## 1. Incident Triage

### Impact Summary

**Ce qu'ont v√©cu les utilisateurs:**

Alors l√†, on a eu droit a un enchainement classique de probleme... Les clients ont commenc√© √† avoir des erreurs 5xx en cascade. Les logins plantaient, il fallait s'y reprendre 2-3 fois pour que √ßa passe. Les appels API timeout ou √©chouaient et les d√©ploiements prenaient de 6 a 8 minutes au lieu des 2-3 habituelles. J'ai aussi vu pas mal d'erreurs "too many connections" c√¥t√© base de donn√©es dans les logs. l'infrastructure fonctionnait par intermittence.

**L'ampleur des d√©g√¢ts:**

Au pic de l'incident vers 09:12 CET, on √©tait √† 5.8% de taux d'erreur 5xx (notre seuil normal c'est 2% max). Plusieurs apps touch√©es sur diff√©rents tenants (tenant-redacted-01, 03, 05 et 08). Et √† partir de 09:15, on a commenc√© √† voir l'exhaustion des slots de connexion sur la base de donn√©es.

### Top 3 Hypotheses (Par ordre de probabilit√©)

**1. √âpuisement des ressources sur les n≈ìuds runtime (C'EST PROBABLEMENT √áA)**

En fouillant dans les logs, j'ai trouv√© le smoking gun : le log shipper a commenc√© √† crier au secours √† 08:56 CET avec des warnings sur le buffer (64-68% d'utilisation) et un CPU qui montait √† 8-11%. Ce probl√®me co√Øncide avec un changement de config √† 08:55. Ensuite une cascade d'OOMKilled √† partir de 09:10 et des √©checs de r√©solution DNS sur les n≈ìuds de production.

Ce qu'il faut v√©rifier d'urgence:
- Le dashboard des n≈ìuds de production pour la pression m√©moire et le taux d'OOM kills
- Comparer l'utilisation des ressources du log provider avant/apr√®s le changement de 08:55
- Regarder la corr√©lation entre les m√©triques CPU/m√©moire au niveau des noeuds et les OOMKills
- Checker le changement CHG-2026-01-21-OBS-0855 en d√©tail

**2. √âpuisement du pool de connexions database (CONS√âQUENCE DU PROBL√àME #1)**

J'ai rep√©r√© des erreurs Postgres FATAL "remaining connection slots are reserved for non-replication superuser connections" qui ont d√©marr√© √† 09:15. L'alerte de connexion s'est d√©clench√©e √† 09:18 avec 2.7 erreurs/sec. Mais bon, pour moi c'est clairement un sympt√¥me: les apps qui crashent et red√©marrent cr√©ent une cascade de connexions.

√Ä v√©rifier:
- Le dashboard Postgres pour voir le % d'utilisation des connexions
- Compter les tentatives de connexion vs les slots disponibles
- Voir si les pics de connexions correspondent aux red√©marrages d'apps

**3. √âchecs des health checks du router (EFFET DE BORD)**

Les logs du router montrent "no healthy upstream", "upstream timeout", "connection termination". C'est clairement une cons√©quence du probl√®me principal sur les noeuds de production.

√Ä v√©rifier:
- Le dashboard router pour voir la r√©partition des erreurs upstream
- Les param√®tres de health check entre le router et le runtime
- Si les erreurs sont localis√©es sur certains n≈ìuds runtime sp√©cifiques

### Plan d'action pour les 15-30 premi√®res minutes

**Actions imm√©diates (T+0 √† T+5):**

1. **√âvaluer l'√©tendue des d√©g√¢ts** (T+0)
   Premi√®re chose, je regarde le dashboard router pour voir le taux de 5xx actuel et quelles apps/tenants sont touch√©s. Ensuite dashboard runtime pour checker le taux d'OOM et la pression m√©moire. Declarer l'incident sur #incidents.

2. **Trouver le d√©clencheur** (T+2)
   Je remonte les changements des 2 derni√®res heures.
   Je remarde CHG-2026-01-21-OBS-0855 √† 08:55 et les cons√©quence √† 09:05.
   Je vais voir ce qu'il y a dans runtime-log-shipper-config-diff.txt.

3. **Mitigation imm√©diate** (T+5)
   On **ROLLBACK le log shipper** √† la version pr√©c√©dente (v1 avec les limites de ressources).
   ```
   kubectl rollout undo daemonset/log-shipper -n runtime-system
   ```
   Le daemonset devrait mettre 5-10 minutes √† se d√©ployer.
   Je surveille le taux d'OOM et la pression m√©moire pendant ce temps.

**Stabilisation (T+5 √† T+15):**

4. **Surveiller la r√©cup√©ration** (T+5-T+15)
   Je garde un oeil sur le taux de 5xx du router.
   Il devrait redescendre sous les 2%.
   Le taux d'OOM devrait tomber √† 0.
   Les erreurs de connexion Postgres devraient passer sous 2 erreurs par seconde.
   Si √ßa ne s'am√©liore pas apr√®s 10 minutes, on passe √† l'hypoth√®se suivante.

5. **√âviter l'effet cascade** (T+10)
   Si les slots Postgres sont toujours satur√©s, petit coup de nettoyage :
   ```sql
   SELECT pg_terminate_backend(pid) FROM pg_stat_activity
   WHERE state = 'idle' AND state_change < now() - interval '5 minutes'
   ```
   Si la pression m√©moire persiste sur certains noeuds, on les draine.

**Communication (T+15 √† T+30):**

6. **Update interne** (T+15)
   Je poste un update sur #incidents et je notifie l'√©quipe Support avec le template pour la status page.

7. **Communication client** (T+20)
   Si on est toujours au-dessus de 1% de 5xx √† T+20, on publie sur la status page.

### Garde-fous importants

Ne pas oublier :
- **SURTOUT PAS** red√©marrer tous les noeuds de production d'un coup (risque de cascade)
- **NE PAS** killer les connexions Postgres manuellement sauf si on est √† >90% d'utilisation
- **NE PAS** scaler les n≈ìuds runtime pendant l'incident (√ßa va juste ajouter de la charge)
- **PENSER √Ä** pr√©server les logs avant le rollback pour le postmortem
- **TOUJOURS** coordonner avec l'equipe avant de faire quelque chose de destructif

### R√©partition des t√¢ches

**Je d√©l√®gue au coll√®gue SRE #2:**
- Surveiller #support pour les nouveaux tickets
- Trier et documenter les clients affect√©s (tenant IDs, app IDs, sympt√¥mes)
- Pr√©parer le r√©sum√© d'impact client pour le postmortem

Cela me permet de me concentrer sur la r√©solution technique pendant qu'on garde une visibilit√© sur l'impact client.

**Je d√©l√®gue √† l'ing√© de garde:**
- Regarder les logs applicatifs pour voir s'il n'y a pas des probl√®mes c√¥t√© app qui aggravent la situation
- Pr√©parer la comm pour freeze les d√©ploiements pendant l'incident

**Je garde pour moi:**
- L'investigation technique et la mitigation (rollback, monitoring)
- La documentation de la timeline
- Les communications internes/externes (status page)

Cela n√©cessite le contexte technique complet et le pouvoir de d√©cision.

---

## 2. Communications

### 2.1 Update interne (Engineering/SRE/Support)

**Channel:** #incidents
**Timing:** T+15 minutes (09:27 CET)

```
[INCIDENT] INC-2026-01-21-ROUTER-5XX ‚Äî On est dessus !

Status: üî¥ En cours ‚Äî Mitigation en place
S√©v√©rit√©: P1 (Impact client)
Dur√©e: Depuis 09:05 CET (~22 minutes)

Ce qui se passe:
‚Ä¢ Erreurs 5xx intermittentes (pic √† 5.8%, seuil normal 2%)
‚Ä¢ Les d√©ploiements rament (6-8 min vs 2-3 min normalement)
‚Ä¢ Erreurs de connexion database
‚Ä¢ Touche plusieurs apps sur region-eu-1/cluster-redacted-01

Root cause:
‚Ä¢ Changement de config du log shipper √† 08:55 qui a vir√© les limites de ressources
‚Ä¢ R√©sultat : pression m√©moire ‚Üí OOMKills ‚Üí effet domino

Ce qu'on fait:
‚úÖ Rollback lanc√© √† 09:22 CET (log shipper retour √† v1)
‚è≥ On surveille : taux 5xx, OOM, connexions DB
‚è≥ Retour √† la normale pr√©vu vers 09:35 CET (~8 minutes)

Actions im√©diates:
‚Ä¢ Engineering: STOP tous les d√©ploiements sur region-eu-1 jusqu'√† nouvel ordre
‚Ä¢ Support: Utilisez le template status page pour les clients (message suivant)
‚Ä¢ SRE: @sre-teammate-2 compile l'impact client

Prochain update: 09:35 CET ou si √ßa bouge
Lead incident: @sre-oncall-1
```

### 2.2 Update client (Status Page)

**Plateforme:** status.scalingo.com
**Timing:** T+20 minutes (09:32 CET)
**Titre:** Taux d'erreur √©lev√© - R√©gion EU

```
Status: Investigation ‚Üí Identifi√© ‚Üí Surveillance

[09:32 CET] Surveillance
Nous avons appliqu√© un fix et surveillons actuellement le retour √† la normale.
Le taux d'erreur des applications diminue progressivement. Les d√©ploiements
reviennent √† leur vitesse habituelle. Retour complet √† la normale attendu
dans les 10 prochaines minutes.

[09:22 CET] Identifi√©
La cause racine a √©t√© identifi√©e - un changement de configuration sur notre
infrastructure. Nous sommes en train de rollback ce changement. Le retour √†
la normale est attendu dans les 15 minutes.

[09:15 CET] Investigation
Nous investiguons actuellement un taux d'erreur √©lev√© affectant les applications
dans notre r√©gion EU (region-eu-1). Vous pouvez rencontrer des erreurs 5xx
intermittentes ou des temps de r√©ponse plus lents que d'habitude. Les d√©ploiements
peuvent prendre plus de temps. Nos √©quipes travaillent activement sur la r√©solution.

Impact:
‚Ä¢ R√©gion affect√©e: EU (region-eu-1)
‚Ä¢ Sympt√¥mes: Erreurs 5xx intermittentes, d√©ploiements lents, timeouts database occasionnels
‚Ä¢ Contournement: Les retry fonctionnent g√©n√©ralement

Updates toutes les 15 minutes jusqu'√† r√©solution.
```

---

## 3. Observability Upgrade

### 3.1 Service Level Indicators (SLIs)

**SLI 1: Taux de succ√®s des requ√™tes**

On mesure le pourcentage de requ√™tes qui renvoient du 2xx/3xx (pas du 5xx).

Comment on le mesure concr√®tement :
- M√©trique: `(sum(router_http_requests_total{code!~"5.."}) / sum(router_http_requests_total)) * 100`
- Source: Les logs d'acc√®s du router, agr√©g√©s dans Prometheus
- Fen√™tre: Rolling window de 5 minutes
- Scope: Par cluster/r√©gion

**SLI 2: Latence des requ√™tes (p95)**

Le 95√®me percentile de la dur√©e des requ√™tes, du router jusqu'√† la r√©ponse.

La mesure :
- M√©trique: `histogram_quantile(0.95, router_request_duration_seconds_bucket)`
- Source: Les histogrammes du router
- Fen√™tre: Rolling 5 minutes
- Scope: Par cluster/r√©gion

**SLI 3: Taux de succ√®s des d√©ploiements**

Le pourcentage de d√©ploiements qui se terminent avec succ√®s dans le SLA (5 minutes). 

Mesure :
- M√©trique: `(count(deploy_completed{result="success",duration<300}) / count(deploy_started)) * 100`
- Source: M√©triques de l'orchestrateur de d√©ploiement
- Fen√™tre: Rolling 1 heure
- Scope: Par cluster/r√©gion

**SLI 4: Disponibilit√© des connexions database**

Le pourcentage de tentatives de connexion qui r√©ussissent. 

Comment on mesure :
- M√©trique: `(sum(pg_connections_successful) / sum(pg_connection_attempts)) * 100`
- Source: M√©triques du pooler Postgres
- Fen√™tre: Rolling 5 minutes
- Scope: Par pg_cluster

### 3.2 Service Level Objectives (SLOs)

**SLO 1: Taux de succ√®s ‚â• 99.5% (fen√™tre 28 jours)**

On vise 99.5% de requ√™tes qui passent (donc max 0.5% d'erreurs 5xx). Sur 28 jours √ßa nous donne un error budget d'environ 3.6 heures par mois.

Seuil d'alerte: Si on d√©passe 2% d'erreurs 5xx pendant 5 minutes, c'est une urgence car l'on consomme notre error budget √† un rythme trop rapide.

**SLO 2: Latence p95 ‚â§ 500ms (fen√™tre 28 jours)**

95% des requ√™tes doivent se terminer en moins de 500ms.

Seuil d'alerte: p95 > 2000ms pendant 10 minutes = d√©gradation s√©v√®re.

**SLO 3: Taux de succ√®s des d√©ploiements ‚â• 99.0% (fen√™tre 7 jours)**

99% des d√©ploiements doivent passer en moins de 5 minutes. 
Reagir vite si les deploiement plante.

Seuil d'alerte: Taux de succ√®s < 95% sur 1 heure

**SLO 4: Disponibilit√© connexions DB ‚â• 99.9% (fen√™tre 28 jours)**

99.9% des tentatives de connexion doivent r√©ussir.

Seuil d'alerte: Taux d'erreur > 2 errors/sec pendant 5 minutes = urgence.

### 3.3 Politique d'alerting

**Les alertes qui declencche une astreinte:**

Conditions pour reagir:
1. Alertes de burn rate des SLO:
   - Taux 5xx > 2% pendant 5 minutes
   - Latence p95 > 2000ms pendant 10 minutes
   - Taux de succ√®s des d√©ploiements < 95% pendant 1 heure
   - Erreurs de connexion DB > 2/sec pendant 5 minutes

2. Infra critique:
   - Utilisation des slots Postgres > 90%
   - Taux d'OOM kill > 5/minute sur les noeuds de production
   - Outage complet du cluster (tous les routers down)

PagerDuty ‚Üí notification #incidents ‚Üí on-call SRE.
Si pas d'ack en 5 minutes, escalade au SRE lead.

**Les alertes a traiter (tickets):**

Conditions:
1. Warnings SLO (pas encore critique):
   - Taux 5xx > 1% pendant 10 minutes
   - Latence p95 > 1000ms pendant 20 minutes
   - Erreurs connexion DB > 1/sec pendant 10 minutes

2. Warnings capacit√©:
   - Pression m√©moire n≈ìuds runtime > 80% pendant 30 minutes
   - Utilisation connexions Postgres > 70%
   - Buffer log shipper > 70% pendant 15 minutes (√áA, √ßa aurait attrap√© notre incident!)

3. Op√©rationnel:
   - Certificat expire dans < 14 jours
   - Disque > 80% utilis√©

Creer un ticket Jira et poste dans #sre-alerts
SLA : review sous 4 heures ouvr√©es.

**R√©duction du bruit:**

1. **Seuil de trafic minimum:** Pas d'alerte 5xx si < 10 req/s
2. **Ne pas declencher si:** L'alerte se r√©sout seulement apr√®s 2x la dur√©e du trigger sous le seuil
3. **Groupage:** On regroupe les applications qui sont touch√©s
4. **Maintenance windows:** On ne repete pas les alerts pendant les changements planifi√©s
5. **Seuils dynamiques:** Pour les m√©triques avec patterns journaliers, on utilise de la d√©tection d'anomalie
6. **Dashboard fatigue:** On track le temps ack et la fr√©quence de page par r√®gle, review trimestriel pour virer/tuner les alertes bruyantes

### 3.4 Dashboards et Runbooks pour l'on-call

**Dashboards:**

1. **Dashboard Incident Response (√Ä CR√âER)**

   Faire un dashbord qui regoupe tout:
   - Router: taux 5xx, RPS, latences p95/p99, breakdown des erreurs
   - Runtime: taux OOM, pression m√©moire, CPU, restarts, erreurs DNS
   - Postgres: utilisation connexions, taux d'erreur, latence queries p95
   - Changements r√©cents: Les 24 derni√®res heures avec liens directs

   Avec des contr√¥les de temps "2 derni√®res heures", "24h", "fen√™tre d'incident" et les alertes qui se superposent automatiquement.

2. **Dashboard Attribution des Ressources (√Ä CR√âER)**

   Pour identifier ce qui prends toute la m√©moire/CPU :
   - Breakdown par noeuds des top containers
   - Utilisation du log shipper vs ses limites configur√©es
   - Containers qui approchent de l'OOM

   √áa nous aurait permis de voir direct que le log shipper etait le prob√®me.

3. **Dashboard Impact Client (am√©liorer l'existant)**

   - Top tenants/apps par taux 5xx
   - Taux de succ√®s et latence par tenant
   - Liens vers les dashboards d√©taill√©s par tenant
   - Volume de tickets support si on a l'int√©gration

**Runbooks:**

1. **Runbook: Router 5xx √©lev√©**

   Il faut am√©liorer celui qu'on a :
   - √âtape 0: Checker le Dashboard Incident Response EN PREMIER
   - √âtape 1: Remonter les changements des 2 derni√®res heures
   - √âtape 2: V√©rifier la sant√© des noeuds runtime (OOM, m√©moire, DNS)
   - √âtape 3: Checker les connexions Postgres
   - √âtape 4: Identifier si c'est localis√© (certains n≈ìuds/apps)
   - Arbre de d√©cision: rollback vs drain vs scale-up
   - Templates de commandes rollback avec safety checks

2. **Runbook: Erreurs connexion Postgres**

   Am√©liorer avec :
   - Pre-check: Est-ce que c'est secondaire √† une instabilit√© runtime ?
   - Query pour voir qui squatte les connexions
   - Nettoyage safe des connexions idle > 5min
   - Proc√©dure d'urgence pour augmenter max_connections

3. **Runbook: Pression m√©moire noeuds runtime (√Ä CR√âER)**

   - Triggers: Pression m√©moire > 85% ou OOM kills > 3/min
   - Identifier les top consommateurs de m√©moire
   - Checker les daemons sans limites (log shipper, agents monitoring)
   - Drainer le n≈ìud si pression > 15 minutes
   - V√©rifier les changements r√©cents sur les daemons

4. **Runbook: Rollback d'urgence (√Ä CR√âER)**

   - Types: ConfigMaps, DaemonSets, Deployments
   - Pre-flight: Backup de la config actuelle, v√©rifier que la cible du rollback existe
   - Commandes: `kubectl rollout undo`, `kubectl rollout status`
   - M√©triques √† surveiller pendant le rollback
   - Crit√®res de d√©cision: quand rollback vs fix forward

5. **Runbook: SLO Breach (√Ä CR√âER)**

   - Liens vers les runbooks par SLO
   - Calcul du burn rate actuel
   - Templates de notification stakeholders
   - Crit√®res pour d√©clencher un postmortem

**Priorit√©s d'impl√©mentation:**

L√† il faut qu'on soit pragmatiques sur ce qu'on peut faire vite :

1. Cette semaine: Dashboard Incident Response et Attribution des Ressources
2. Dans 2 semaines: Am√©lioration des runbooks existants
3. Dans 1 mois: Nouveaux runbooks et dashboards SLO

---

## 4. Postmortem

### 4.1 Impact, D√©tection, Timeline

**L'impact r√©el:**

L'incident a dur√© environ 55 minutes avec des erreurs √©lev√©es (09:05‚Äì10:00 CET), la phase la plus critique √©tait entre 09:10 et 09:30. R√©gion region-eu-1, cluster-redacted-01.

C√¥t√© exp√©rience utilisateur, on a tap√© un pic √† 5.8% d'erreurs 5xx (normalement on est sous 0.1%). Si je fais le calcul rapide avec notre trafic habituel de 100 req/s, √ßa nous fait environ 3500 requ√™tes qui ont √©chou√© pendant le pic. Les users avaient des √©checs de login, des timeouts d'API, les d√©ploiements qui tra√Ænaient (6-8 min au lieu de 2-3). Et bien s√ªr les erreurs de connexion database qui commen√ßaient √† appara√Ætre.

Impact client : 4 tickets haute priorit√© (ST-2026-01121-0421, 0423, 0426, 0429), les tenants majeurs touch√©s (tenant-redacted-01, -03, -05, -08 - des gros clients). R√©cup√©ration partielle vers 09:30, retour complet √† 10:00.

Impact business : On risque un casser la SLA pour notre client 99.96% (s'ils ont d√©j√† eu >17 minutes de down ce mois-ci). Sans compter l'impact sur la confiance client et les ~6 heures-personnes pass√©es sur l'incident.

**La d√©tection:**

Premier sympt√¥me : un client qui ouvre un ticket support √† 09:13 CET. Notre alerte automatique s'est d√©clench√©e juste avant √† 09:12 pour le taux de 5xx.

On a mis ~7 minutes entre le d√©but r√©el de l'impact (09:05) et l'alerte. Pourquoi ce d√©lai ? Notre alerte attend que le taux de 5xx soit >2% pendant 5 minutes d'affil√©e. Entre 09:05 et 09:10 √ßa montait progressivement, puis √† 09:10-09:12 on √©tait au-dessus du seuil. Et on n'avait AUCUNE alerte proactive sur les ressources des noeuds de prod - pas de monitoring de la pression m√©moire ou du taux d'OOM.

**La timeline d√©taill√©e:**

| Heure (CET) | Ce qui s'est pass√© | Source |
|------------|---------------------|--------|
| 08:45 | Le changement CHG-2026-01-21-OBS-0855 est approuv√© | Change record |
| 08:55 | D√©but du rollout de la config log shipper (v1‚Üív2, suppression des limites de ressources) | Change record |
| 08:56 | Premiers warnings du log shipper: buffer √† 64%, CPU √† 8% | Runtime logs |
| 08:58 | Activation du bundle dashboard/alertes | Change record |
| 09:03 | Fin du rollout log shipper | Change record |
| 09:05 | Premiers sympt√¥mes clients (estimation depuis les tickets) | Support tickets |
| 09:08 | Log shipper monte √† 11% CPU, buffer 68% | Runtime logs |
| 09:10 | Premiers containers OOMKilled (app-redacted-05) | Runtime logs |
| 09:10 | Premiers √©checs de r√©solution DNS | Runtime logs |
| 09:10 | Premi√®res erreurs upstream router (502/504) | Router logs |
| 09:12 | **ALERTE:** Taux 5xx > 2% (5.8% en r√©alit√©) | Pager |
| 09:13 | Premier ticket support (ST-2026-01121-0421) | Support |
| 09:15 | D√©but saturation slots connexion Postgres (erreurs FATAL) | Postgres logs |
| 09:15‚Äì09:19 | Avalanche de tickets support | Support |
| 09:18 | **ALERTE:** Erreurs connexion Postgres > 2/sec (2.7/sec) | Pager |
| 09:22 | Lancement du rollback (log shipper v2‚Üív1) | *[Action SRE hypoth√©tique]* |
| 09:30 | R√©cup√©ration partielle, taux 5xx en baisse | Router logs |
| 09:40‚Äì10:00 | Erreurs r√©siduelles intermittentes, r√©cup√©ration progressive | Tous les logs |
| 10:00 | Fin de l'incident, services stables | Tous les logs |

### 4.2 Root Cause et Facteurs Contributifs

**La cause racine:**

Le changement CHG-2026-01-21-OBS-0855. On a supprim√© les limites de ressources (CPU et m√©moire) du daemonset log shipper sur les noeuds de prod. Sans limites, les log shippers se sont mis √† consommer un max de ressources, cr√©ant une pression m√©moire sur les noeuds.

L'effet domino √©tait pr√©visible :
1. Pression m√©moire ‚Üí OOMKills des containers d'application
2. OOMKills ‚Üí Red√©marrages des apps
3. Red√©marrages ‚Üí Temp√™te de connexions vers Postgres
4. Temp√™te de connexions ‚Üí Saturation des slots Postgres
5. Instabilit√© des n≈ìuds ‚Üí √âchecs de r√©solution DNS (contention syst√®me)
6. √âchecs upstream ‚Üí Erreurs 5xx au router (plus d'upstreams healthy)

Le diff probl√©matique dans runtime-log-shipper-config-diff.txt :
```yaml
-  resources:
-    cpu_limit: "200m"
-    mem_limit: "256Mi"
-    cpu_request: "100m"
-    mem_request: "128Mi"
```
Ces lignes ont √©t√© supprim√©es, permettant une consommation illimit√©e.

**Les facteurs contributifs:**

1. **Review de changement insuffisante:**
   La suppression des limites de ressources n'√©tait pas explicitement mentionn√©e dans le change record. On parlait surtout d'ajout de dashboards/alertes, la "simplification de config" √©tait minimis√©e. Pas d'√©valuation d'impact sur les ressources demand√©e.

   **Fix:** Ajouter une section obligatoire "Impact Ressources" dans le template de changement.

2. **Pas de d√©ploiement canary:**
   Le log shipper a √©t√© d√©ploy√© sur tous les n≈ìuds d'un coup via le daemonset. Pas de rollout progressif genre 10% ‚Üí 50% ‚Üí 100%.

   **Fix:** Impl√©menter une strat√©gie canary pour les daemonsets d'infrastructure.

3. **Alertes proactives manquantes:**
   Rien pour surveiller l'utilisation des ressources du log shipper, la pression m√©moire des noeufs runtime, ou le taux d'OOM kills. On a attendu que √ßa se manifeste par des 5xx au router.

   **Fix:** Ajouter les alertes d√©crites en Section 3.3.

4. **Tuning d'alerte insuffisant:**
   L'alerte router 5xx demande 5 minutes de soutenu (bien pour √©viter le bruit, mais √ßa ajoute 5 min de d√©lai). Pas d'alerte "fast burn" pour les d√©gradations s√©v√®res.

   **Fix:** Ajouter une alerte fast-burn pour 5xx > 5% pendant 1 minute.

5. **Manque de visibilit√© sur les ressources:**
   Le dashboard runtime existe mais personne ne le regardait pendant le changement. Pas de "health check" automatis√© post-changement.

   **Fix:** Impl√©menter un monitoring automatique post-changement (voir 4.3).

6. **Blast radius non contenu:**
   Le changement a affect√© tout le cluster d'un coup. Pas de rollout par zone ou par noeuds.

   **Fix:** Canary rollouts zone-aware pour les changements d'infrastructure.

### 4.3 Actions Correctives

#### Imm√©diat (0‚Äì1 semaine)

| Action | Responsable | Deadline | Status |
|--------|-------------|----------|--------|
| Rollback log shipper vers v1 (avec limites) | SRE on-call | 2026-01-21 (Fait) | ‚úÖ Fait |
| Ajouter alertes utilisation log shipper (CPU >50%, Mem >200Mi) | SRE - @sre-lead-redacted-02 | 2026-01-23 | üî≤ √Ä faire |
| Ajouter alerte pression m√©moire n≈ìuds (>80% 30min) | SRE - @sre-lead-redacted-02 | 2026-01-23 | üî≤ √Ä faire |
| Ajouter alerte taux OOM kill (>3/min 5min) | SRE - @sre-lead-redacted-02 | 2026-01-23 | üî≤ √Ä faire |
| Cr√©er Dashboard Incident Response | SRE - @sre-oncall-redacted-01 | 2026-01-25 | üî≤ √Ä faire |
| Documenter runbook rollback d'urgence | SRE - @sre-oncall-redacted-01 | 2026-01-24 | üî≤ √Ä faire |
| Fixer config log shipper v2 (remettre les limites) | Platform - @platform-observability-redacted | 2026-01-26 | üî≤ √Ä faire |

#### Court terme (1‚Äì4 semaines)

| Action | Responsable | Deadline | Status |
|--------|-------------|----------|--------|
| Ajouter champ "Impact Ressources" obligatoire au template de changement | SRE Lead | 2026-02-07 | üî≤ √Ä faire |
| Impl√©menter strat√©gie canary daemonset (10%‚Üí50%‚Üí100%, 10min soak) | Platform | 2026-02-14 | üî≤ √Ä faire |
| Am√©liorer runbook router 5xx avec checks runtime | SRE | 2026-02-07 | üî≤ √Ä faire |
| Am√©liorer runbook erreurs connexion Postgres | SRE | 2026-02-07 | üî≤ √Ä faire |
| Cr√©er runbook Pression M√©moire Runtime | SRE | 2026-02-10 | üî≤ √Ä faire |
| Cr√©er Dashboard Attribution Ressources | SRE | 2026-02-10 | üî≤ √Ä faire |
| Ajouter alerte fast-burn: 5xx >5% 1min | SRE | 2026-02-14 | üî≤ √Ä faire |
| Re-d√©ployer log shipper v2 corrig√© en canary | Platform | 2026-02-21 | üî≤ √Ä faire |

#### Long terme (1‚Äì3 mois)

| Action | Responsable | Deadline | Status |
|--------|-------------|----------|--------|
| Monitoring automatique post-changement (watchdog 30min) | Platform + SRE | 2026-03-15 | üî≤ √Ä faire |
| Dashboard tracking SLO et error budget | SRE | 2026-03-31 | üî≤ √Ä faire |
| Canary rollouts zone-aware pour l'infra | Platform | 2026-04-15 | üî≤ √Ä faire |
| Chaos engineering: tests pression m√©moire en staging | SRE | 2026-03-31 | üî≤ √Ä faire |
| Audit complet des daemonsets pour les limites manquantes | Platform | 2026-02-28 | üî≤ √Ä faire |
| Dashboard impact client | SRE | 2026-03-15 | üî≤ √Ä faire |

### 4.4 Documentation et Runbooks

**Documentation √† cr√©er:**

1. **Guide de R√©ponse aux Incidents**
   - Fichier: `docs/runbooks/incident-response-guide.md`
   - Contenu: Workflow standard, templates de comm, escalation, template postmortem
   - Pourquoi: On a besoin de standardiser notre r√©ponse, surtout quand on est stress√©s

2. **Best Practices Change Management**
   - Fichier: `docs/change-management/best-practices.md`
   - Contenu: √âvaluation d'impact ressources, strat√©gies canary, proc√©dures rollback, validation post-changement
   - Pourquoi: Pour √©viter de reproduire ce genre d'incident li√© √† la config

3. **Architecture des N≈ìuds Runtime** (am√©liorer l'existant)
   - Fichier: `docs/architecture/runtime-nodes.md`
   - Ajouter: Section sur les limites de ressources des daemonsets, gestion pression m√©moire, comportement OOMKiller
   - Pourquoi: Pour que tout le monde comprenne l'impact des changements au niveau n≈ìud

**Documentation √† mettre √† jour:**

1. **README Stack Observability**
   - Fichier: `docs/observability/README.md`
   - Update: Config ressources log shipper, guidelines capacity planning
   - Pourquoi: Documenter ce qu'on a appris sur les besoins en ressources du log shipper

2. **Guide Connection Pooling Postgres**
   - Fichier: `docs/databases/postgres-connection-pooling.md`
   - Update: Section troubleshooting saturation slots, proc√©dures d'urgence
   - Pourquoi: Pour mitiger plus vite les incidents li√©s aux connexions

3. **Playbook On-Call**
   - Fichier: `docs/oncall/playbook.md`
   - Update: Liens vers les nouveaux dashboards et runbooks
   - Pourquoi: Rendre les nouveaux outils d√©couvrables pour l'on-call

**Runbooks √† cr√©er/updater:**
- Voir Section 3.4 pour la liste d√©taill√©e

---

## 5. Gestion Client

### Contexte
- **Client:** Client important avec database business
- **SLA:** 99.96% uptime mensuel
- **Demande:** Remboursement via le TAM
- **√âquipe commerciale:** Demande notre avis SRE

### Mon analyse

**Calcul du SLA:**

Avec un objectif de 99.96% mensuel, on a droit √† maximum (1 - 0.9996) √ó 30 jours √ó 24h √ó 60min = **17.28 minutes** de downtime par mois.

**L'impact r√©el pour ce client:**

En regardant les logs et les tickets, leur database a bien √©t√© touch√©e :
- Fen√™tre d'incident: 09:05‚Äì10:00 CET (55 minutes au total)
- Impact maximum: 09:10‚Äì09:30 CET (20 minutes)
- Sympt√¥mes: Erreurs de connexion, 5xx intermittents

**√âvaluation du downtime:**

D√©gradation partielle du cote technique, pas un outage complet.
Les retry fonctionnaient souvent. Si on calcule strictement :
- En downtime complet: 0 minutes (le service n'√©tait pas compl√®tement down)
- En taux de succ√®s des requ√™tes:
  - Pic √† 5.8% d'erreurs pendant ~20 minutes
  - Disponibilit√© pendant le pic: 94.2%
  - Impact sur la dispo mensuelle: (5.8% √ó 20min) / (30j √ó 24h √ó 60min) = 0.0027%
  - **Disponibilit√© mensuelle projet√©e: 99.9973%** (toujours au-dessus du SLA de 99.96%)

Attention, √ßa suppose qu'on n'a pas d'autres incidents ce mois-ci.

### Ma recommandation √† l'√©quipe commerciale

**R√©ponse imm√©diate (dans les 4 heures):**

```
Objet: RE: Demande de remboursement - Incident INC-2026-01-21

Bonjour [Nom du TAM],

Merci de nous avoir contact√©s concernant la d√©gradation de service du 21/01/2026.
Je comprends tout √† fait votre pr√©occupation et je veux vous fournir une √©valuation
technique transparente de la situation.

R√©sum√© de l'incident:
‚Ä¢ Fen√™tre: 09:05‚Äì10:00 CET (55 minutes), pic d'impact 09:10‚Äì09:30 CET
‚Ä¢ Impact: Erreurs intermittentes (pic √† 5.8%), temps de r√©ponse d√©grad√©s,
  quelques erreurs de connexion database
‚Ä¢ Cause: Un changement de configuration sur notre infrastructure qui a √©t√©
  compl√®tement rollback
‚Ä¢ Status actuel: R√©solu, services stables

Analyse d'impact SLA pour [Nom du Client]:
‚Ä¢ Votre SLA: 99.96% de disponibilit√© mensuelle (17.28 minutes de downtime autoris√©)
‚Ä¢ Cet incident: D√©gradation partielle (erreurs intermittentes, pas outage complet)
‚Ä¢ Impact calcul√© sur la disponibilit√©: ~0.003% de r√©duction
‚Ä¢ Votre disponibilit√© actuelle ce mois-ci: 99.997% (au-dessus du SLA)

Cependant, je reconnais que m√™me une d√©gradation partielle impacte vos op√©rations.

R√©solution propos√©e:
[Si EN DESSOUS du SLA]: Conform√©ment aux termes de notre SLA, nous allons
√©mettre un cr√©dit de [X]% de vos frais mensuels de database, trait√© sous 5 jours.

[Si AU-DESSUS du SLA - geste commercial]: Bien que cet incident n'ait pas
d√©pass√© votre SLA, nous valorisons notre partenariat et souhaitons vous offrir
un cr√©dit commercial de 10% sur les frais de database de ce mois.

Mesures pr√©ventives que nous mettons en place:
‚Ä¢ Monitoring am√©lior√© pour d√©tection pr√©coce (alertes d√©j√† ajout√©es)
‚Ä¢ Process de change management renforc√© pour les mises √† jour d'infrastructure
‚Ä¢ Strat√©gie de d√©ploiement canary pour limiter l'impact des futurs changements

Prochaines √©tapes:
‚Ä¢ Je suis disponible pour un call si vous souhaitez discuter des d√©tails
  techniques et de nos actions correctives
‚Ä¢ Notre document postmortem sera disponible sous 5 jours si vous souhaitez
  une revue technique d√©taill√©e

N'h√©sitez pas si vous voulez en discuter. Nous prenons ces incidents tr√®s au
s√©rieux et nous nous engageons √† maintenir la haute fiabilit√© que vous attendez.

Cordialement,
[Nom du SRE Lead]
```

**Conseils pour l'√©quipe commerciale:**

**√Ä FAIRE:**
- Reconna√Ætre l'impact et montrer de l'empathie
- √ätre transparent sur les d√©tails techniques (√ßa construit la confiance)
- Proposer une r√©solution proactivement
- Mettre en avant les actions correctives (montrer qu'on apprend)
- Proposer un call (montre l'engagement)

**√Ä NE PAS FAIRE:**
- Rejeter la demande parce que "techniquement le SLA n'est pas breach"
- Promettre que √ßa n'arrivera plus jamais (irr√©aliste)
- Bl√¢mer le client pour une mauvaise compr√©hension du SLA

**Ma recommandation sur le remboursement:**

**Option 2: Cr√©dit commercial de 10-15%**

Pourquoi ? Le cout est moindre et l'impact sur le client qui se sentira √©cout√© et valoris√©.

Si client strat√©gique : support prioritaire en plus.

**Points √† v√©rifier avant de valider:**
- Demander au TAM l'impact r√©el ressenti par le client
- Checker nos logs pour voir si leur DB √©tait plus impact√©e que la moyenne
- Relire les termes exacts du contrat SLA

**Ma reco finale:**

Approuvez le cr√©dit de 10-15%. Un client content vaut plus que le co√ªt du cr√©dit.

---

## Note m√©thodologique

Ce document a √©t√© pr√©par√© suite √† l'analyse approfondie de multiples sources d'incident : logs d√©taill√©s, m√©triques de monitoring, alertes syst√®me, et records de changement. L'analyse s'appuie sur mon exp√©rience en gestion d'incidents sur des infrastructures cloud-native similaires, avec une attention particuli√®re port√©e √† la corr√©lation temporelle des √©v√©nements et √† l'identification des relations de causalit√© entre les diff√©rents sympt√¥mes observ√©s.

Je me suis permis d'utiliser Claude pour recuperer les formules de calculs n√©casaire a a la partir 2 ( Pour l'observabilit√© )

Les recommandations d'am√©lioration de l'observabilit√© sont bas√©es sur les best practices SRE actuelles et adapt√©es au contexte sp√©cifique de cette plateforme. La strat√©gie de communication client √©quilibre les consid√©rations techniques, contractuelles et relationnelles pour maintenir la confiance tout en restant transparent sur les faits.

---

**Document pr√©par√© par:** √âquipe SRE
**Date:** 2026-01-21
**Incident:** INC-2026-01-21-ROUTER-5XX
**Status:** Postmortem termin√© ‚Äî Actions correctives en cours
